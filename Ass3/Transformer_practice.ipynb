{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa35b36",
   "metadata": {
    "id": "0aa35b36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## STAT8021 / STAT8307\n",
    "### Assignment 3: Transformer Mechanics, Application, and Pre-training/Fine-tuning Analysis\n",
    "### DUE: April 27, 2025, Sunday, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qIU8i6pb8Iir",
   "metadata": {
    "id": "qIU8i6pb8Iir"
   },
   "source": [
    "## 1. Understanding Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pASQaC4H8ePz",
   "metadata": {
    "id": "pASQaC4H8ePz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-uvJ2PlH8sTD",
   "metadata": {
    "id": "-uvJ2PlH8sTD"
   },
   "source": [
    "### Transformer: Multi-head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MM5T2mBG_Kcj",
   "metadata": {
    "id": "MM5T2mBG_Kcj"
   },
   "source": [
    "\n",
    "\n",
    "#### Q1 (a) Query, Key, Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAmRRaDj_P9k",
   "metadata": {
    "id": "LAmRRaDj_P9k"
   },
   "source": [
    "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d_1}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V_i,K_i,Q_i \\in \\mathbb{R}^{d_1\\times d/h}$ to map our input $X$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iXCCiRks_Wz0",
   "metadata": {
    "id": "iXCCiRks_Wz0"
   },
   "source": [
    "\\begin{align}\n",
    "v_i = V_iX\\ \\ i \\in \\{1,\\dots,h\\}\\\\\n",
    "k_i = K_iX\\ \\ i \\in \\{1,\\dots,h\\}\\\\\n",
    "q_i = Q_iX\\ \\ i \\in \\{1,\\dots,h\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85OOZD2pdCNy",
   "metadata": {
    "id": "85OOZD2pdCNy"
   },
   "source": [
    "where $i$ refers to the $i$-th head and $h$ is the number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pNat9vIsSium",
   "metadata": {
    "id": "pNat9vIsSium"
   },
   "outputs": [],
   "source": [
    "def get_multihead_qkv(query, key, value, embed_dim, n_heads):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - query: Input data to be used as the query, of shape (N, S, hidden_dim)\n",
    "        - key: Input data to be used as the key, of shape (N, T, hidden_dim)\n",
    "        - value: Input data to be used as the value, of shape (N, T, hidden_dim)\n",
    "        - embed_dim: The embedding dimension of q,k,v (d in the formula)\n",
    "        - n_heads: the number of heads\n",
    "        Note: In the shape definitions above, N is the batch size, S is the source\n",
    "        sequence length, T is the target sequence length, and hidden_dim is the hidden dimension of X (d1 in the formula).\n",
    "    Returns:\n",
    "        - output: a tuple containg query, key, value with shapes of (N, H, S, head_dim), (N, H, T, head_dim), (N, H, T, head_dim) respectively\n",
    "    \"\"\"\n",
    "    N, S, E = query.shape\n",
    "    N, T, E = value.shape\n",
    "    assert embed_dim % n_heads == 0\n",
    "    \n",
    "    head_dim = embed_dim // n_heads\n",
    "    # Notes:\n",
    "    #  1) Define your projections using nn.Linear() and initialize them following the order q,k,v\n",
    "    #  2) You'll want to split your shape from (N, T, embed_dim) into (N, T, H, head_dim),\n",
    "        #     where H is the number of heads.\n",
    "    #  3) Tensor.view() and Tensor.permute() might help\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Write your code here\n",
    "    q_proj = nn.Linear(E, n_heads * head_dim)\n",
    "    k_proj = nn.Linear(E, n_heads * head_dim)\n",
    "    v_proj = nn.Linear(E, n_heads * head_dim)\n",
    "\n",
    "    q = q_proj(query).view(N, S, n_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    k = k_proj(key).view(N, T, n_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    v = v_proj(value).view(N, T, n_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return q,k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SXwFEj1MZ60f",
   "metadata": {
    "id": "SXwFEj1MZ60f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of query is torch.Size([1, 2, 3, 4]).\n",
      "The L2 norm of query is 2.7053.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "embed_dim = 8 #d\n",
    "hidden_dim = 8 #d1\n",
    "n_heads = 2\n",
    "data = torch.randn(batch_size, sequence_length, hidden_dim)\n",
    "q, k, v = get_multihead_qkv(data, data, data, embed_dim, n_heads)\n",
    "print('The shape of query is {}.'.format(q.shape))\n",
    "print('The L2 norm of query is {:.4f}.'.format(torch.linalg.norm(q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994wvNQH97Vr",
   "metadata": {
    "id": "994wvNQH97Vr"
   },
   "source": [
    "#### Q1 (b) Multi-Headed Scaled Dot-Product Attention\n",
    "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ideXR49_kr",
   "metadata": {
    "id": "00ideXR49_kr"
   },
   "source": [
    "\\begin{equation} \\label{qkv_eqn}\n",
    "A_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mjsHC_U_c3rZ",
   "metadata": {
    "id": "mjsHC_U_c3rZ"
   },
   "outputs": [],
   "source": [
    "def calculate_multihead_attention(q, k, attn_mask=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - q: Multi-head query with the shape of (N, H, S, head_dim)\n",
    "        - k: Multi-head key with the shape of (N, H, T, head_dim)\n",
    "        - attn_mask (if provided): Array of shape (S, T) where attn_mask[i,j] == 0 indicates token\n",
    "          j in the key/value should not influence token i in the query output.\n",
    "        - Note: head_dim refers to embed_dim/n_heads\n",
    "    Returns:\n",
    "        - attention_weights: attention tensor with shape of (N, H, S, T)\n",
    "    \"\"\"\n",
    "    # Notes:\n",
    "    #  1) You need to transpose k\n",
    "    #  2) You need to set scores to '-inf' where mask==0. Tensor.masked_fill() might help.\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Write your code here    \n",
    "    d_h = q.shape[3]  \n",
    "    \n",
    "    qk_proj = torch.matmul(q, k.transpose(-2, -1))\n",
    "    if attn_mask is not None:\n",
    "        qk_proj = qk_proj.masked_fill(attn_mask, - torch.inf)\n",
    "    \n",
    "    qk_proj = qk_proj / math.sqrt(d_h)\n",
    "    \n",
    "    attention_weights = F.softmax(qk_proj, dim=-1)\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xawwntia2jRg",
   "metadata": {
    "id": "xawwntia2jRg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of attention is torch.Size([1, 2, 3, 3]).\n",
      "The L2 norm of self-attention is 1.4747.\n",
      "The L2 norm of masked_self_attn_output is 2.0186.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(456)\n",
    "# Create a 0/1 mask where 0 means mask out, 1 means keep\n",
    "mask = torch.randn(sequence_length, sequence_length) < 0.5 # ~50% are 0s (masked)\n",
    "self_attn_output = calculate_multihead_attention(q, k)\n",
    "masked_self_attn_output = calculate_multihead_attention(q, k, attn_mask=mask)\n",
    "print('The shape of attention is {}.'.format(self_attn_output.shape))\n",
    "print('The L2 norm of self-attention is {:.4f}.'.format(torch.linalg.norm(self_attn_output)))\n",
    "print('The L2 norm of masked_self_attn_output is {:.4f}.'.format(torch.linalg.norm(masked_self_attn_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8mgT1FAL5CXz",
   "metadata": {
    "id": "8mgT1FAL5CXz"
   },
   "source": [
    "#### Q1 (c) Final outputs and Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tKVfPzG77CZ",
   "metadata": {
    "id": "7tKVfPzG77CZ"
   },
   "source": [
    "Now we have got our attention $A_i$, and each head's output could be calculated using the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HPKNRfzZ-U5l",
   "metadata": {
    "id": "HPKNRfzZ-U5l"
   },
   "source": [
    "\\begin{equation}\n",
    "Y_i = A_i(XV_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q-Du5zIv-Cpc",
   "metadata": {
    "id": "Q-Du5zIv-Cpc"
   },
   "source": [
    "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XbS9uvjGcy25",
   "metadata": {
    "id": "XbS9uvjGcy25"
   },
   "source": [
    "\n",
    "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqboXv0i8iIR",
   "metadata": {
    "id": "vqboXv0i8iIR"
   },
   "source": [
    "\\begin{equation} \\label{qkvdropout_eqn}\n",
    "Y_i = \\text{dropout}\\bigg(A_i\\bigg)(XV_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qHaE5Z9m_T2w",
   "metadata": {
    "id": "qHaE5Z9m_T2w"
   },
   "source": [
    "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D8SPCDBH_Xwo",
   "metadata": {
    "id": "D8SPCDBH_Xwo"
   },
   "source": [
    "\\begin{equation}\n",
    "Y = [Y_1;\\dots;Y_h]W\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CArjQvm4_bbB",
   "metadata": {
    "id": "CArjQvm4_bbB"
   },
   "source": [
    "where $W \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "x8cdfEzB9-sT",
   "metadata": {
    "id": "x8cdfEzB9-sT"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A model layer which implements a simplified version of masked attention, as\n",
    "    introduced by \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Construct a new MultiHeadAttention layer.\n",
    "\n",
    "        Inputs:\n",
    "         - embed_dim: Dimension of the token embedding\n",
    "         - num_heads: Number of attention heads\n",
    "         - dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        # Linear layers for Q, K, V projections (input dim = output dim = embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final linear projection layer\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Dropout layer for attention weights\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Store dimensions\n",
    "        self.n_head = num_heads\n",
    "        self.emd_dim = embed_dim\n",
    "        self.head_dim = self.emd_dim // self.n_head\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Calculate the masked attention output for the provided data, computing\n",
    "        all attention heads in parallel.\n",
    "\n",
    "        In the shape definitions below, N is the batch size, S is the source\n",
    "        sequence length, T is the target sequence length, and E is the embedding\n",
    "        dimension.\n",
    "\n",
    "        Inputs:\n",
    "        - query: Input data to be used as the query, of shape (N, S, E)\n",
    "        - key: Input data to be used as the key, of shape (N, T, E)\n",
    "        - value: Input data to be used as the value, of shape (N, T, E)\n",
    "        - attn_mask: Array of shape (S, T) where mask[i,j] == 0 indicates token\n",
    "          i in the source should not influence token j in the target.\n",
    "\n",
    "        Returns:\n",
    "        - output: Tensor of shape (N, S, E) giving the weighted combination of\n",
    "          data in value according to the attention weights calculated using key\n",
    "          and query.\n",
    "        \"\"\"\n",
    "        N, S, E = query.shape\n",
    "        N, T, E = value.shape\n",
    "\n",
    "        # Notes:\n",
    "        #  1) Please do not directly call the functions defined above. Instead, write your code step by step.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Write your code here\n",
    "        q = self.query(query).view(N, S, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.key(key).view(N, T, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.value(value).view(N, T, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_proj = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            qk_proj = qk_proj.masked_fill(attn_mask == 0, - torch.inf)\n",
    "        \n",
    "        qk_proj = qk_proj / self.head_dim ** 0.5\n",
    "        \n",
    "        attention_weights = F.softmax(qk_proj, dim=-1)\n",
    "\n",
    "        Y = torch.matmul(self.attn_drop(attention_weights), v).permute(0, 2, 1, 3).reshape(N, T, self.n_head * self.head_dim)\n",
    "        \n",
    "        output = self.proj(Y)\n",
    "        \n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2IJKehv69mKT",
   "metadata": {
    "id": "2IJKehv69mKT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn_output error:  0.0003772742211599121\n",
      "masked_self_attn_output error:  0.0001526367643724865\n",
      "attn_output error:  0.00035224630317522767\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "attn = MultiHeadAttention(embed_dim, num_heads=2)\n",
    "\n",
    "# Self-attention.\n",
    "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "self_attn_output = attn(query=data, key=data, value=data)\n",
    "\n",
    "# Masked self-attention.\n",
    "mask = torch.randn(sequence_length, sequence_length) < 0.5\n",
    "masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)\n",
    "\n",
    "# Attention using two inputs.\n",
    "other_data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "attn_output = attn(query=data, key=other_data, value=other_data)\n",
    "\n",
    "expected_self_attn_output = np.asarray([[\n",
    "[-0.2494,  0.1396,  0.4323, -0.2411, -0.1547,  0.2329, -0.1936,\n",
    "          -0.1444],\n",
    "         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,\n",
    "          -0.2476],\n",
    "         [-0.0625,  0.1503,  0.7572, -0.3974, -0.1681,  0.2168, -0.2478,\n",
    "          -0.3038]]])\n",
    "\n",
    "expected_masked_self_attn_output = np.asarray([[\n",
    "[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,\n",
    "          -0.3019],\n",
    "         [-0.1013,  0.3111,  0.5783, -0.3248, -0.3842,  0.1482, -0.3628,\n",
    "          -0.1496],\n",
    "         [-0.2071,  0.1669,  0.7097, -0.3152, -0.3136,  0.2520, -0.2774,\n",
    "          -0.2208]]])\n",
    "\n",
    "expected_attn_output = np.asarray([[\n",
    "[-0.1980,  0.4083,  0.1968, -0.3477,  0.0321,  0.4258, -0.8972,\n",
    "          -0.2744],\n",
    "         [-0.1603,  0.4155,  0.2295, -0.3485, -0.0341,  0.3929, -0.8248,\n",
    "          -0.2767],\n",
    "         [-0.0908,  0.4113,  0.3017, -0.3539, -0.1020,  0.3784, -0.7189,\n",
    "          -0.2912]]])\n",
    "\n",
    "print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))\n",
    "print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))\n",
    "print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iHWEwsVSFSSy",
   "metadata": {
    "id": "iHWEwsVSFSSy"
   },
   "source": [
    "Checker: The correct implementation will give an error no more than `e-3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64WbmBYxMMZe",
   "metadata": {
    "id": "64WbmBYxMMZe"
   },
   "source": [
    "### Transformer: Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BqSx5fdCJ0Li",
   "metadata": {
    "id": "BqSx5fdCJ0Li"
   },
   "source": [
    "#### Q1 (d) Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PE0kZXOmKXOi",
   "metadata": {
    "id": "PE0kZXOmKXOi"
   },
   "source": [
    "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
    "\n",
    "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
    "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "YyNFwZ2oKm5D",
   "metadata": {
    "id": "YyNFwZ2oKm5D"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes information about the positions of the tokens in the sequence. In\n",
    "    this case, the layer has no learnable parameters, since it is a simple\n",
    "    function of sines and cosines.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Construct the PositionalEncoding layer.\n",
    "\n",
    "        Inputs:\n",
    "         - embed_dim: the size of the embed dimension\n",
    "         - dropout: the dropout value\n",
    "         - max_len: the maximum possible length of the incoming sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        assert embed_dim % 2 == 0\n",
    "        # Create an array with a \"batch dimension\" of 1 (which will broadcast\n",
    "        # across all examples in the batch).\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        # Notes:\n",
    "        #  1) Construct the positional encoding array as described above.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Write your code here\n",
    "        for i in range(max_len):\n",
    "            for j in range(embed_dim):\n",
    "                if j % 2 == 0:\n",
    "                    pe[0][i][j] = math.sin(i * 10000 ** (- j / embed_dim))\n",
    "                else:\n",
    "                    pe[0][i][j] = math.cos(i * 10000 ** (- (j-1) / embed_dim))\n",
    "                    \n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Make sure the positional encodings will be saved with the model\n",
    "        # parameters (mostly for completeness).\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Element-wise add positional embeddings to the input sequence.\n",
    "\n",
    "        Inputs:\n",
    "         - x: the sequence fed to the positional encoder model, of shape\n",
    "              (N, S, D), where N is the batch size, S is the sequence length and\n",
    "              D is embed dim\n",
    "        Returns:\n",
    "         - output: the input sequence + positional encodings, of shape (N, S, D)\n",
    "        \"\"\"\n",
    "        N, S, D = x.shape\n",
    "        # Notes:\n",
    "        #  1) Index into your array of positional encodings, and add the appropriate ones to the input sequence.\n",
    "        #  2) Don't forget to apply dropout afterward.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Write your code here\n",
    "        output = self.attn_drop(x + self.pe[:, :S, :])\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qHA6CtpMLzCQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1743474636651,
     "user": {
      "displayName": "YIHANG CHEN",
      "userId": "04571529654271604828"
     },
     "user_tz": -480
    },
    "id": "qHA6CtpMLzCQ",
    "outputId": "40b711f9-df36-4aeb-aada-a7692c9ee510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe_output error:  0.00010421011374914356\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 2\n",
    "embed_dim = 6\n",
    "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "\n",
    "pos_encoder = PositionalEncoding(embed_dim)\n",
    "output = pos_encoder(data)\n",
    "\n",
    "expected_pe_output = np.asarray([[[-1.2340,  1.1127,  1.6978, -0.0865, -0.0000,  1.2728],\n",
    "                                  [ 0.9028, -0.4781,  0.5535,  0.8133,  1.2644,  1.7034]]])\n",
    "\n",
    "print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BcjG6OdhL4oq",
   "metadata": {
    "id": "BcjG6OdhL4oq"
   },
   "source": [
    "Checker: The correct implementation will give an error no more than `e-3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ac67e",
   "metadata": {},
   "source": [
    "## 2. Applying Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8431c695",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (2.13.2)\n",
      "Requirement already satisfied: evaluate in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (6.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: responses<0.19 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5efb39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset parquet (C:/Users/23629/.cache/huggingface/datasets/parquet/ag_news-9af2a5926861d22a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 250.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ag_news_dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077eca8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5569e8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-63a729b30640687c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-4626180bb011e782.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-387c2fe96c3db8c6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-25bdc38418fb4a58.arrow\n"
     ]
    }
   ],
   "source": [
    "# Just take the first 100 tokens for speed/running on cpu\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:100]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "# Take 1024 random examples for train and 128 validation\n",
    "small_ag_news_dataset = DatasetDict(\n",
    "    train=ag_news_dataset['train'].shuffle(seed=1111).select(range(1024)).map(truncate),\n",
    "    val=ag_news_dataset['test'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10677ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'India and Pakistan balk at bold Kashmir peace plan Pakistani President Pervez Musharraf this week urged steps to end the bitter dispute.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ag_news_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74bd0a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Nortel warns of lower Q3 revenue TORONTO - Nortel Networks warned Thursday its third-quarter revenue will be below the \\\\$2.6 billion US preliminary unaudited revenues it reported for the second quarter.',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ag_news_dataset['val'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1f2b06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"World\", \n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00004780",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q2 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5401cf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-660febad63e88154.arrow\n",
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-d66dcfb19e8d85d1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 3, 1]),\n",
       " 'input_ids': tensor([[ 101, 2634, 1998,  ...,    0,    0,    0],\n",
       "         [ 101, 3042, 2194,  ...,    0,    0,    0],\n",
       "         [ 101, 2148, 4420,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(token):\n",
    "    return tokenizer(token[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "small_tokenized_dataset = small_ag_news_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "small_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "# print the frist 3 processed samples\n",
    "small_tokenized_dataset['train'][:3]\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1575",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q2 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac5c1ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training process:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [07:59<00:00,  7.49s/it]\n",
      "Testing process:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc = 0.7217, test acc = 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [07:27<00:00,  6.99s/it]\n",
      "Testing process:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train acc = 0.9209, test acc = 0.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [07:28<00:00,  7.00s/it]\n",
      "Testing process:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train acc = 0.9658, test acc = 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 3\n",
    "bsz = 8\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset[\"train\"], batch_size=bsz, shuffle=True)\n",
    "test_dataloader = DataLoader(small_tokenized_dataset[\"val\"], batch_size=bsz)\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_warmup_steps = int(0.1 * num_epochs * len(train_dataloader))\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                               num_training_steps=num_epochs * len(train_dataloader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train and evaluate your model\n",
    "    model.train()\n",
    "    train_correct, train_total = 0, 0\n",
    "    for batch in tqdm(train_dataloader,desc=\"Training process:\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        if 'label' in batch:\n",
    "            batch['labels'] = batch.pop('label')        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        train_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        train_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Testing process:\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if 'label' in batch:\n",
    "                batch['labels'] = batch.pop('label')            \n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            test_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            test_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "\n",
    "        \n",
    "    # print the training process\n",
    "    print(\"Epoch {}: train acc = {:.4f}, test acc = {:.4f}\".format(epoch + 1, train_acc, test_acc))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48a65c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q2 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c04cde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class of news 'In an exciting match last night, the Los Angeles Lakers defeated the Brooklyn Nets 115-110. Lakers' LeBron James made a comeback after missing several games due to injury and scored 25 points while teammate Anthony Davis added 28 points. Nets' star player Kevin Durant scored 32 points but couldn't lead his team to victory.' is Sports \n",
      "\n",
      "The class of news 'Scientists have discovered a new species of dinosaur that roamed the earth 80 million years ago. The species, named Almatherium, was found in Uzbekistan and is believed to be an ancestor of the modern-day armadillo. The discovery sheds new light on the evolution of mammals and their relationship with dinosaurs.' is Sci/Tech \n",
      "\n",
      "The class of news 'The United Nations has called for an immediate ceasefire in Yemen as the country faces a growing humanitarian crisis. The UN's special envoy for Yemen, Martin Griffiths, urged all parties to end the violence and engage in peace talks. The conflict has left millions of Yemenis at risk of famine and disease.' is World \n",
      "\n",
      "The class of news 'Amazon has announced that it will be opening its first fulfillment center in New Zealand, creating more than 500 new jobs. The center will be located in Auckland and is expected to open in 2022. This move will allow Amazon to expand its operations in the region and improve delivery times for customers.' is Business \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chatgpt_generated_news = [\n",
    "    \"In an exciting match last night, the Los Angeles Lakers defeated the Brooklyn Nets 115-110. Lakers' LeBron James made a comeback after missing several games due to injury and scored 25 points while teammate Anthony Davis added 28 points. Nets' star player Kevin Durant scored 32 points but couldn't lead his team to victory.\",\n",
    "    \"Scientists have discovered a new species of dinosaur that roamed the earth 80 million years ago. The species, named Almatherium, was found in Uzbekistan and is believed to be an ancestor of the modern-day armadillo. The discovery sheds new light on the evolution of mammals and their relationship with dinosaurs.\",\n",
    "    \"The United Nations has called for an immediate ceasefire in Yemen as the country faces a growing humanitarian crisis. The UN's special envoy for Yemen, Martin Griffiths, urged all parties to end the violence and engage in peace talks. The conflict has left millions of Yemenis at risk of famine and disease.\",\n",
    "    \"Amazon has announced that it will be opening its first fulfillment center in New Zealand, creating more than 500 new jobs. The center will be located in Auckland and is expected to open in 2022. This move will allow Amazon to expand its operations in the region and improve delivery times for customers.\",\n",
    "]\n",
    "prediction_label = []\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "# test your finetuned model on chatgpt_genreated_news\n",
    "model.eval()\n",
    "for news in chatgpt_generated_news:\n",
    "    inputs = tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    prediction_label.append(predicted_class)\n",
    "\n",
    "for ids, prediction_label in enumerate(prediction_label):\n",
    "    print(f\"The class of news '{chatgpt_generated_news[ids]}' is {id2label[prediction_label]} \\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242704e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q2 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f96607",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 25.0/25.0 [00:00<00:00, 5.00kB/s]\n",
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\23629\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading vocab.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.16MB/s]\n",
      "Downloading merges.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 965kB/s]\n",
      "Downloading tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:01<00:00, 1.33MB/s]\n",
      "Downloading config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 67.6kB/s]\n",
      "Downloading model.safetensors: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499M/499M [02:05<00:00, 3.96MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training process:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [15:07<00:00,  7.09s/it]\n",
      "Testing process:: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:30<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc = 0.7285, test acc = 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [15:11<00:00,  7.12s/it]\n",
      "Testing process:: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:30<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train acc = 0.9131, test acc = 0.8672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [15:11<00:00,  7.12s/it]\n",
      "Testing process:: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:30<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train acc = 0.9492, test acc = 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess_function(token):\n",
    "    return tokenizer(token[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "small_tokenized_dataset = small_ag_news_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "small_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 3\n",
    "bsz = 8\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset[\"train\"], batch_size=bsz, shuffle=True)\n",
    "test_dataloader = DataLoader(small_tokenized_dataset[\"val\"], batch_size=bsz)\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_warmup_steps = int(0.1 * num_epochs * len(train_dataloader))\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                               num_training_steps=num_epochs * len(train_dataloader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train and evaluate your model\n",
    "    model.train()\n",
    "    train_correct, train_total = 0, 0\n",
    "    for batch in tqdm(train_dataloader,desc=\"Training process:\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        if 'label' in batch:\n",
    "            batch['labels'] = batch.pop('label')        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        train_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        train_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Testing process:\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if 'label' in batch:\n",
    "                batch['labels'] = batch.pop('label')            \n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            test_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            test_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "\n",
    "        \n",
    "    # print the training process\n",
    "    print(\"Epoch {}: train acc = {:.4f}, test acc = {:.4f}\".format(epoch + 1, train_acc, test_acc))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d653a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
