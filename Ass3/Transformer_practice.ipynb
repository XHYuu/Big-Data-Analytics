{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa35b36",
   "metadata": {
    "id": "0aa35b36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## STAT8021 / STAT8307\n",
    "### Assignment 3: Transformer Mechanics, Application, and Pre-training/Fine-tuning Analysis\n",
    "### DUE: April 27, 2025, Sunday, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qIU8i6pb8Iir",
   "metadata": {
    "id": "qIU8i6pb8Iir"
   },
   "source": [
    "## 1. Understanding Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pASQaC4H8ePz",
   "metadata": {
    "id": "pASQaC4H8ePz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-uvJ2PlH8sTD",
   "metadata": {
    "id": "-uvJ2PlH8sTD"
   },
   "source": [
    "### Transformer: Multi-head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MM5T2mBG_Kcj",
   "metadata": {
    "id": "MM5T2mBG_Kcj"
   },
   "source": [
    "\n",
    "\n",
    "#### Q1 (a) Query, Key, Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAmRRaDj_P9k",
   "metadata": {
    "id": "LAmRRaDj_P9k"
   },
   "source": [
    "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d_1}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V_i,K_i,Q_i \\in \\mathbb{R}^{d_1\\times d/h}$ to map our input $X$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iXCCiRks_Wz0",
   "metadata": {
    "id": "iXCCiRks_Wz0"
   },
   "source": [
    "\\begin{align}\n",
    "v_i = V_iX\\ \\ i \\in \\{1,\\dots,h\\}\\\\\n",
    "k_i = K_iX\\ \\ i \\in \\{1,\\dots,h\\}\\\\\n",
    "q_i = Q_iX\\ \\ i \\in \\{1,\\dots,h\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85OOZD2pdCNy",
   "metadata": {
    "id": "85OOZD2pdCNy"
   },
   "source": [
    "where $i$ refers to the $i$-th head and $h$ is the number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pNat9vIsSium",
   "metadata": {
    "id": "pNat9vIsSium"
   },
   "outputs": [],
   "source": [
    "def get_multihead_qkv(query, key, value, embed_dim, n_heads):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - query: Input data to be used as the query, of shape (N, S, hidden_dim)\n",
    "        - key: Input data to be used as the key, of shape (N, T, hidden_dim)\n",
    "        - value: Input data to be used as the value, of shape (N, T, hidden_dim)\n",
    "        - embed_dim: The embedding dimension of q,k,v (d in the formula)\n",
    "        - n_heads: the number of heads\n",
    "        Note: In the shape definitions above, N is the batch size, S is the source\n",
    "        sequence length, T is the target sequence length, and hidden_dim is the hidden dimension of X (d1 in the formula).\n",
    "    Returns:\n",
    "        - output: a tuple containg query, key, value with shapes of (N, H, S, head_dim), (N, H, T, head_dim), (N, H, T, head_dim) respectively\n",
    "    \"\"\"\n",
    "    N, S, E = query.shape\n",
    "    N, T, E = value.shape\n",
    "    assert embed_dim % n_heads == 0\n",
    "    \n",
    "    head_dim = embed_dim // n_heads\n",
    "    # Notes:\n",
    "    #  1) Define your projections using nn.Linear() and initialize them following the order q,k,v\n",
    "    #  2) You'll want to split your shape from (N, T, embed_dim) into (N, T, H, head_dim),\n",
    "        #     where H is the number of heads.\n",
    "    #  3) Tensor.view() and Tensor.permute() might help\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Write your code here\n",
    "    q_proj = nn.Linear(E, n_heads * head_dim)\n",
    "    k_proj = nn.Linear(E, n_heads * head_dim)\n",
    "    v_proj = nn.Linear(E, n_heads * head_dim)\n",
    "\n",
    "    q = q_proj(query).view(N, S, n_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    k = k_proj(key).view(N, T, n_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    v = v_proj(value).view(N, T, n_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    return q,k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SXwFEj1MZ60f",
   "metadata": {
    "id": "SXwFEj1MZ60f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of query is torch.Size([1, 2, 3, 4]).\n",
      "The L2 norm of query is 2.7053.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "embed_dim = 8 #d\n",
    "hidden_dim = 8 #d1\n",
    "n_heads = 2\n",
    "data = torch.randn(batch_size, sequence_length, hidden_dim)\n",
    "q, k, v = get_multihead_qkv(data, data, data, embed_dim, n_heads)\n",
    "print('The shape of query is {}.'.format(q.shape))\n",
    "print('The L2 norm of query is {:.4f}.'.format(torch.linalg.norm(q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994wvNQH97Vr",
   "metadata": {
    "id": "994wvNQH97Vr"
   },
   "source": [
    "#### Q1 (b) Multi-Headed Scaled Dot-Product Attention\n",
    "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ideXR49_kr",
   "metadata": {
    "id": "00ideXR49_kr"
   },
   "source": [
    "\\begin{equation} \\label{qkv_eqn}\n",
    "A_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mjsHC_U_c3rZ",
   "metadata": {
    "id": "mjsHC_U_c3rZ"
   },
   "outputs": [],
   "source": [
    "def calculate_multihead_attention(q, k, attn_mask=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - q: Multi-head query with the shape of (N, H, S, head_dim)\n",
    "        - k: Multi-head key with the shape of (N, H, T, head_dim)\n",
    "        - attn_mask (if provided): Array of shape (S, T) where attn_mask[i,j] == 0 indicates token\n",
    "          j in the key/value should not influence token i in the query output.\n",
    "        - Note: head_dim refers to embed_dim/n_heads\n",
    "    Returns:\n",
    "        - attention_weights: attention tensor with shape of (N, H, S, T)\n",
    "    \"\"\"\n",
    "    # Notes:\n",
    "    #  1) You need to transpose k\n",
    "    #  2) You need to set scores to '-inf' where mask==0. Tensor.masked_fill() might help.\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Write your code here    \n",
    "    d_h = q.shape[3]  \n",
    "    \n",
    "    qk_proj = torch.matmul(q, k.transpose(-2, -1))\n",
    "    if attn_mask is not None:\n",
    "        qk_proj = qk_proj.masked_fill(attn_mask, - torch.inf)\n",
    "    \n",
    "    qk_proj = qk_proj / math.sqrt(d_h)\n",
    "    \n",
    "    attention_weights = F.softmax(qk_proj, dim=-1)\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xawwntia2jRg",
   "metadata": {
    "id": "xawwntia2jRg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of attention is torch.Size([1, 2, 3, 3]).\n",
      "The L2 norm of self-attention is 1.4747.\n",
      "The L2 norm of masked_self_attn_output is 2.0186.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(456)\n",
    "# Create a 0/1 mask where 0 means mask out, 1 means keep\n",
    "mask = torch.randn(sequence_length, sequence_length) < 0.5 # ~50% are 0s (masked)\n",
    "self_attn_output = calculate_multihead_attention(q, k)\n",
    "masked_self_attn_output = calculate_multihead_attention(q, k, attn_mask=mask)\n",
    "print('The shape of attention is {}.'.format(self_attn_output.shape))\n",
    "print('The L2 norm of self-attention is {:.4f}.'.format(torch.linalg.norm(self_attn_output)))\n",
    "print('The L2 norm of masked_self_attn_output is {:.4f}.'.format(torch.linalg.norm(masked_self_attn_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8mgT1FAL5CXz",
   "metadata": {
    "id": "8mgT1FAL5CXz"
   },
   "source": [
    "#### Q1 (c) Final outputs and Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tKVfPzG77CZ",
   "metadata": {
    "id": "7tKVfPzG77CZ"
   },
   "source": [
    "Now we have got our attention $A_i$, and each head's output could be calculated using the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HPKNRfzZ-U5l",
   "metadata": {
    "id": "HPKNRfzZ-U5l"
   },
   "source": [
    "\\begin{equation}\n",
    "Y_i = A_i(XV_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q-Du5zIv-Cpc",
   "metadata": {
    "id": "Q-Du5zIv-Cpc"
   },
   "source": [
    "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XbS9uvjGcy25",
   "metadata": {
    "id": "XbS9uvjGcy25"
   },
   "source": [
    "\n",
    "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqboXv0i8iIR",
   "metadata": {
    "id": "vqboXv0i8iIR"
   },
   "source": [
    "\\begin{equation} \\label{qkvdropout_eqn}\n",
    "Y_i = \\text{dropout}\\bigg(A_i\\bigg)(XV_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qHaE5Z9m_T2w",
   "metadata": {
    "id": "qHaE5Z9m_T2w"
   },
   "source": [
    "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D8SPCDBH_Xwo",
   "metadata": {
    "id": "D8SPCDBH_Xwo"
   },
   "source": [
    "\\begin{equation}\n",
    "Y = [Y_1;\\dots;Y_h]W\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CArjQvm4_bbB",
   "metadata": {
    "id": "CArjQvm4_bbB"
   },
   "source": [
    "where $W \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "x8cdfEzB9-sT",
   "metadata": {
    "id": "x8cdfEzB9-sT"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A model layer which implements a simplified version of masked attention, as\n",
    "    introduced by \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Construct a new MultiHeadAttention layer.\n",
    "\n",
    "        Inputs:\n",
    "         - embed_dim: Dimension of the token embedding\n",
    "         - num_heads: Number of attention heads\n",
    "         - dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        # Linear layers for Q, K, V projections (input dim = output dim = embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final linear projection layer\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Dropout layer for attention weights\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Store dimensions\n",
    "        self.n_head = num_heads\n",
    "        self.emd_dim = embed_dim\n",
    "        self.head_dim = self.emd_dim // self.n_head\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Calculate the masked attention output for the provided data, computing\n",
    "        all attention heads in parallel.\n",
    "\n",
    "        In the shape definitions below, N is the batch size, S is the source\n",
    "        sequence length, T is the target sequence length, and E is the embedding\n",
    "        dimension.\n",
    "\n",
    "        Inputs:\n",
    "        - query: Input data to be used as the query, of shape (N, S, E)\n",
    "        - key: Input data to be used as the key, of shape (N, T, E)\n",
    "        - value: Input data to be used as the value, of shape (N, T, E)\n",
    "        - attn_mask: Array of shape (S, T) where mask[i,j] == 0 indicates token\n",
    "          i in the source should not influence token j in the target.\n",
    "\n",
    "        Returns:\n",
    "        - output: Tensor of shape (N, S, E) giving the weighted combination of\n",
    "          data in value according to the attention weights calculated using key\n",
    "          and query.\n",
    "        \"\"\"\n",
    "        N, S, E = query.shape\n",
    "        N, T, E = value.shape\n",
    "\n",
    "        # Notes:\n",
    "        #  1) Please do not directly call the functions defined above. Instead, write your code step by step.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Write your code here\n",
    "        q = self.query(query).view(N, S, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.key(key).view(N, T, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.value(value).view(N, T, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk_proj = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            qk_proj = qk_proj.masked_fill(attn_mask == 0, - torch.inf)\n",
    "        \n",
    "        qk_proj = qk_proj / self.head_dim ** 0.5\n",
    "        \n",
    "        attention_weights = F.softmax(qk_proj, dim=-1)\n",
    "\n",
    "        Y = torch.matmul(self.attn_drop(attention_weights), v).permute(0, 2, 1, 3).reshape(N, T, self.n_head * self.head_dim)\n",
    "        \n",
    "        output = self.proj(Y)\n",
    "        \n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2IJKehv69mKT",
   "metadata": {
    "id": "2IJKehv69mKT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn_output error:  0.0003772742211599121\n",
      "masked_self_attn_output error:  0.0001526367643724865\n",
      "attn_output error:  0.00035224630317522767\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "attn = MultiHeadAttention(embed_dim, num_heads=2)\n",
    "\n",
    "# Self-attention.\n",
    "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "self_attn_output = attn(query=data, key=data, value=data)\n",
    "\n",
    "# Masked self-attention.\n",
    "mask = torch.randn(sequence_length, sequence_length) < 0.5\n",
    "masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)\n",
    "\n",
    "# Attention using two inputs.\n",
    "other_data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "attn_output = attn(query=data, key=other_data, value=other_data)\n",
    "\n",
    "expected_self_attn_output = np.asarray([[\n",
    "[-0.2494,  0.1396,  0.4323, -0.2411, -0.1547,  0.2329, -0.1936,\n",
    "          -0.1444],\n",
    "         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,\n",
    "          -0.2476],\n",
    "         [-0.0625,  0.1503,  0.7572, -0.3974, -0.1681,  0.2168, -0.2478,\n",
    "          -0.3038]]])\n",
    "\n",
    "expected_masked_self_attn_output = np.asarray([[\n",
    "[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,\n",
    "          -0.3019],\n",
    "         [-0.1013,  0.3111,  0.5783, -0.3248, -0.3842,  0.1482, -0.3628,\n",
    "          -0.1496],\n",
    "         [-0.2071,  0.1669,  0.7097, -0.3152, -0.3136,  0.2520, -0.2774,\n",
    "          -0.2208]]])\n",
    "\n",
    "expected_attn_output = np.asarray([[\n",
    "[-0.1980,  0.4083,  0.1968, -0.3477,  0.0321,  0.4258, -0.8972,\n",
    "          -0.2744],\n",
    "         [-0.1603,  0.4155,  0.2295, -0.3485, -0.0341,  0.3929, -0.8248,\n",
    "          -0.2767],\n",
    "         [-0.0908,  0.4113,  0.3017, -0.3539, -0.1020,  0.3784, -0.7189,\n",
    "          -0.2912]]])\n",
    "\n",
    "print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))\n",
    "print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))\n",
    "print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iHWEwsVSFSSy",
   "metadata": {
    "id": "iHWEwsVSFSSy"
   },
   "source": [
    "Checker: The correct implementation will give an error no more than `e-3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64WbmBYxMMZe",
   "metadata": {
    "id": "64WbmBYxMMZe"
   },
   "source": [
    "### Transformer: Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BqSx5fdCJ0Li",
   "metadata": {
    "id": "BqSx5fdCJ0Li"
   },
   "source": [
    "#### Q1 (d) Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PE0kZXOmKXOi",
   "metadata": {
    "id": "PE0kZXOmKXOi"
   },
   "source": [
    "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
    "\n",
    "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
    "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "YyNFwZ2oKm5D",
   "metadata": {
    "id": "YyNFwZ2oKm5D"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes information about the positions of the tokens in the sequence. In\n",
    "    this case, the layer has no learnable parameters, since it is a simple\n",
    "    function of sines and cosines.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Construct the PositionalEncoding layer.\n",
    "\n",
    "        Inputs:\n",
    "         - embed_dim: the size of the embed dimension\n",
    "         - dropout: the dropout value\n",
    "         - max_len: the maximum possible length of the incoming sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        assert embed_dim % 2 == 0\n",
    "        # Create an array with a \"batch dimension\" of 1 (which will broadcast\n",
    "        # across all examples in the batch).\n",
    "        pe = torch.zeros(1, max_len, embed_dim)\n",
    "        # Notes:\n",
    "        #  1) Construct the positional encoding array as described above.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Write your code here\n",
    "        for i in range(max_len):\n",
    "            for j in range(embed_dim):\n",
    "                if j % 2 == 0:\n",
    "                    pe[0][i][j] = math.sin(i * 10000 ** (- j / embed_dim))\n",
    "                else:\n",
    "                    pe[0][i][j] = math.cos(i * 10000 ** (- (j-1) / embed_dim))\n",
    "                    \n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Make sure the positional encodings will be saved with the model\n",
    "        # parameters (mostly for completeness).\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Element-wise add positional embeddings to the input sequence.\n",
    "\n",
    "        Inputs:\n",
    "         - x: the sequence fed to the positional encoder model, of shape\n",
    "              (N, S, D), where N is the batch size, S is the sequence length and\n",
    "              D is embed dim\n",
    "        Returns:\n",
    "         - output: the input sequence + positional encodings, of shape (N, S, D)\n",
    "        \"\"\"\n",
    "        N, S, D = x.shape\n",
    "        # Notes:\n",
    "        #  1) Index into your array of positional encodings, and add the appropriate ones to the input sequence.\n",
    "        #  2) Don't forget to apply dropout afterward.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Write your code here\n",
    "        output = self.attn_drop(x + self.pe[:, :S, :])\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qHA6CtpMLzCQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1743474636651,
     "user": {
      "displayName": "YIHANG CHEN",
      "userId": "04571529654271604828"
     },
     "user_tz": -480
    },
    "id": "qHA6CtpMLzCQ",
    "outputId": "40b711f9-df36-4aeb-aada-a7692c9ee510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe_output error:  0.00010421011374914356\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 2\n",
    "embed_dim = 6\n",
    "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "\n",
    "pos_encoder = PositionalEncoding(embed_dim)\n",
    "output = pos_encoder(data)\n",
    "\n",
    "expected_pe_output = np.asarray([[[-1.2340,  1.1127,  1.6978, -0.0865, -0.0000,  1.2728],\n",
    "                                  [ 0.9028, -0.4781,  0.5535,  0.8133,  1.2644,  1.7034]]])\n",
    "\n",
    "print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BcjG6OdhL4oq",
   "metadata": {
    "id": "BcjG6OdhL4oq"
   },
   "source": [
    "Checker: The correct implementation will give an error no more than `e-3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16716941",
   "metadata": {
    "id": "16716941",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Applying Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431c695",
   "metadata": {
    "id": "8431c695",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5efb39",
   "metadata": {
    "id": "eb5efb39",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ag_news_dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077eca8c",
   "metadata": {
    "id": "077eca8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ag_news_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5569e8d",
   "metadata": {
    "id": "a5569e8d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Just take the first 100 tokens for speed/running on cpu\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:100]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "# Take 1024 random examples for train and 128 validation\n",
    "small_ag_news_dataset = DatasetDict(\n",
    "    train=ag_news_dataset['train'].shuffle(seed=1111).select(range(1024)).map(truncate),\n",
    "    val=ag_news_dataset['test'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10677ac",
   "metadata": {
    "id": "a10677ac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_ag_news_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd0a24",
   "metadata": {
    "id": "74bd0a24"
   },
   "outputs": [],
   "source": [
    "small_ag_news_dataset['val'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f2b06",
   "metadata": {
    "id": "ed1f2b06",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00004780",
   "metadata": {
    "id": "00004780",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5401cf2",
   "metadata": {
    "id": "b5401cf2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "# print the frist 3 processed samples\n",
    "small_tokenized_dataset['train'][:3]\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1575",
   "metadata": {
    "id": "550a1575",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5c1ef",
   "metadata": {
    "id": "bac5c1ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# This session might take a long time (≈1 hour), please be patient\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train and evaluate your model\n",
    "\n",
    "    # print the training process\n",
    "    print(\"Epoch {}: train acc = {:.4f}, validation acc = {:.4f}\".format(epoch + 1, train_acc, validation_acc))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48a65c",
   "metadata": {
    "id": "9c48a65c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c04cde",
   "metadata": {
    "id": "d6c04cde",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatgpt_generated_news = [\n",
    "    \"In an exciting match last night, the Los Angeles Lakers defeated the Brooklyn Nets 115-110. Lakers' LeBron James made a comeback after missing several games due to injury and scored 25 points while teammate Anthony Davis added 28 points. Nets' star player Kevin Durant scored 32 points but couldn't lead his team to victory.\",\n",
    "    \"Scientists have discovered a new species of dinosaur that roamed the earth 80 million years ago. The species, named Almatherium, was found in Uzbekistan and is believed to be an ancestor of the modern-day armadillo. The discovery sheds new light on the evolution of mammals and their relationship with dinosaurs.\",\n",
    "    \"The United Nations has called for an immediate ceasefire in Yemen as the country faces a growing humanitarian crisis. The UN's special envoy for Yemen, Martin Griffiths, urged all parties to end the violence and engage in peace talks. The conflict has left millions of Yemenis at risk of famine and disease.\",\n",
    "    \"Amazon has announced that it will be opening its first fulfillment center in New Zealand, creating more than 500 new jobs. The center will be located in Auckland and is expected to open in 2022. This move will allow Amazon to expand its operations in the region and improve delivery times for customers.\",\n",
    "]\n",
    "prediction_label = []\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "# test your finetuned model on chatgpt_genreated_news\n",
    "\n",
    "\n",
    "# print the predictions for chatgpt_genreated_news\n",
    "print(prediction_label)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242704e7",
   "metadata": {
    "id": "242704e7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Q2 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f96607",
   "metadata": {
    "id": "37f96607",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This session might take a long time (≈1.5 hours), please be patient\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train and evaluate your model\n",
    "\n",
    "    # print the training process\n",
    "    print(\"Epoch {}: train acc = {:.4f}, validation acc = {:.4f}\".format(epoch + 1, train_acc, validation_acc))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
