{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa35b36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## STAT8021 / STAT8307\n",
    "### Assignment 3: Language Modeling with Transformer Basics\n",
    "### DUE: April 18, 2025, Friday, 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16716941",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transformer Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8431c695",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (2.13.2)\n",
      "Requirement already satisfied: evaluate in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (6.3.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: responses<0.19 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5efb39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset parquet (C:/Users/23629/.cache/huggingface/datasets/parquet/ag_news-9af2a5926861d22a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 250.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ag_news_dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077eca8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5569e8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-63a729b30640687c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-4626180bb011e782.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-387c2fe96c3db8c6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-25bdc38418fb4a58.arrow\n"
     ]
    }
   ],
   "source": [
    "# Just take the first 100 tokens for speed/running on cpu\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:100]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "# Take 1024 random examples for train and 128 validation\n",
    "small_ag_news_dataset = DatasetDict(\n",
    "    train=ag_news_dataset['train'].shuffle(seed=1111).select(range(1024)).map(truncate),\n",
    "    val=ag_news_dataset['test'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10677ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'India and Pakistan balk at bold Kashmir peace plan Pakistani President Pervez Musharraf this week urged steps to end the bitter dispute.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ag_news_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74bd0a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Nortel warns of lower Q3 revenue TORONTO - Nortel Networks warned Thursday its third-quarter revenue will be below the \\\\$2.6 billion US preliminary unaudited revenues it reported for the second quarter.',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ag_news_dataset['val'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1f2b06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"World\", \n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00004780",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5401cf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-660febad63e88154.arrow\n",
      "Loading cached processed dataset at C:\\Users\\23629\\.cache\\huggingface\\datasets\\parquet\\ag_news-9af2a5926861d22a\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-d66dcfb19e8d85d1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 3, 1]),\n",
       " 'input_ids': tensor([[ 101, 2634, 1998,  ...,    0,    0,    0],\n",
       "         [ 101, 3042, 2194,  ...,    0,    0,    0],\n",
       "         [ 101, 2148, 4420,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(token):\n",
    "    return tokenizer(token[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "small_tokenized_dataset = small_ag_news_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "small_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "# print the frist 3 processed samples\n",
    "small_tokenized_dataset['train'][:3]\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1575",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q1 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac5c1ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training process:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [07:59<00:00,  7.49s/it]\n",
      "Testing process:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc = 0.7217, test acc = 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [07:27<00:00,  6.99s/it]\n",
      "Testing process:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train acc = 0.9209, test acc = 0.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [07:28<00:00,  7.00s/it]\n",
      "Testing process:: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:27<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train acc = 0.9658, test acc = 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 3\n",
    "bsz = 8\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset[\"train\"], batch_size=bsz, shuffle=True)\n",
    "test_dataloader = DataLoader(small_tokenized_dataset[\"val\"], batch_size=bsz)\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_warmup_steps = int(0.1 * num_epochs * len(train_dataloader))\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                               num_training_steps=num_epochs * len(train_dataloader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train and evaluate your model\n",
    "    model.train()\n",
    "    train_correct, train_total = 0, 0\n",
    "    for batch in tqdm(train_dataloader,desc=\"Training process:\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        if 'label' in batch:\n",
    "            batch['labels'] = batch.pop('label')        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        train_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        train_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Testing process:\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if 'label' in batch:\n",
    "                batch['labels'] = batch.pop('label')            \n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            test_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            test_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "\n",
    "        \n",
    "    # print the training process\n",
    "    print(\"Epoch {}: train acc = {:.4f}, test acc = {:.4f}\".format(epoch + 1, train_acc, test_acc))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48a65c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q1 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c04cde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class of news 'In an exciting match last night, the Los Angeles Lakers defeated the Brooklyn Nets 115-110. Lakers' LeBron James made a comeback after missing several games due to injury and scored 25 points while teammate Anthony Davis added 28 points. Nets' star player Kevin Durant scored 32 points but couldn't lead his team to victory.' is Sports \n",
      "\n",
      "The class of news 'Scientists have discovered a new species of dinosaur that roamed the earth 80 million years ago. The species, named Almatherium, was found in Uzbekistan and is believed to be an ancestor of the modern-day armadillo. The discovery sheds new light on the evolution of mammals and their relationship with dinosaurs.' is Sci/Tech \n",
      "\n",
      "The class of news 'The United Nations has called for an immediate ceasefire in Yemen as the country faces a growing humanitarian crisis. The UN's special envoy for Yemen, Martin Griffiths, urged all parties to end the violence and engage in peace talks. The conflict has left millions of Yemenis at risk of famine and disease.' is World \n",
      "\n",
      "The class of news 'Amazon has announced that it will be opening its first fulfillment center in New Zealand, creating more than 500 new jobs. The center will be located in Auckland and is expected to open in 2022. This move will allow Amazon to expand its operations in the region and improve delivery times for customers.' is Business \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chatgpt_generated_news = [\n",
    "    \"In an exciting match last night, the Los Angeles Lakers defeated the Brooklyn Nets 115-110. Lakers' LeBron James made a comeback after missing several games due to injury and scored 25 points while teammate Anthony Davis added 28 points. Nets' star player Kevin Durant scored 32 points but couldn't lead his team to victory.\",\n",
    "    \"Scientists have discovered a new species of dinosaur that roamed the earth 80 million years ago. The species, named Almatherium, was found in Uzbekistan and is believed to be an ancestor of the modern-day armadillo. The discovery sheds new light on the evolution of mammals and their relationship with dinosaurs.\",\n",
    "    \"The United Nations has called for an immediate ceasefire in Yemen as the country faces a growing humanitarian crisis. The UN's special envoy for Yemen, Martin Griffiths, urged all parties to end the violence and engage in peace talks. The conflict has left millions of Yemenis at risk of famine and disease.\",\n",
    "    \"Amazon has announced that it will be opening its first fulfillment center in New Zealand, creating more than 500 new jobs. The center will be located in Auckland and is expected to open in 2022. This move will allow Amazon to expand its operations in the region and improve delivery times for customers.\",\n",
    "]\n",
    "prediction_label = []\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Write your code here\n",
    "\n",
    "# test your finetuned model on chatgpt_genreated_news\n",
    "model.eval()\n",
    "for news in chatgpt_generated_news:\n",
    "    inputs = tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    prediction_label.append(predicted_class)\n",
    "\n",
    "for ids, prediction_label in enumerate(prediction_label):\n",
    "    print(f\"The class of news '{chatgpt_generated_news[ids]}' is {id2label[prediction_label]} \\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242704e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Q1 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f96607",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 25.0/25.0 [00:00<00:00, 5.00kB/s]\n",
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\23629\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading vocab.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.16MB/s]\n",
      "Downloading merges.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 965kB/s]\n",
      "Downloading tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:01<00:00, 1.33MB/s]\n",
      "Downloading config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 67.6kB/s]\n",
      "Downloading model.safetensors: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499M/499M [02:05<00:00, 3.96MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training process:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [15:07<00:00,  7.09s/it]\n",
      "Testing process:: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:30<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc = 0.7285, test acc = 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [15:11<00:00,  7.12s/it]\n",
      "Testing process:: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:30<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train acc = 0.9131, test acc = 0.8672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training process:: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [15:11<00:00,  7.12s/it]\n",
      "Testing process:: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:30<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train acc = 0.9492, test acc = 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess_function(token):\n",
    "    return tokenizer(token[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "small_tokenized_dataset = small_ag_news_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "small_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 3\n",
    "bsz = 8\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset[\"train\"], batch_size=bsz, shuffle=True)\n",
    "test_dataloader = DataLoader(small_tokenized_dataset[\"val\"], batch_size=bsz)\n",
    "\n",
    "# Define your model. optimizer, hyper-parameter and etc.\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_warmup_steps = int(0.1 * num_epochs * len(train_dataloader))\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                               num_training_steps=num_epochs * len(train_dataloader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train and evaluate your model\n",
    "    model.train()\n",
    "    train_correct, train_total = 0, 0\n",
    "    for batch in tqdm(train_dataloader,desc=\"Training process:\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        if 'label' in batch:\n",
    "            batch['labels'] = batch.pop('label')        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        train_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        train_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Testing process:\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if 'label' in batch:\n",
    "                batch['labels'] = batch.pop('label')            \n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            test_correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            test_total += batch[\"labels\"].size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "\n",
    "        \n",
    "    # print the training process\n",
    "    print(\"Epoch {}: train acc = {:.4f}, test acc = {:.4f}\".format(epoch + 1, train_acc, test_acc))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d653a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
